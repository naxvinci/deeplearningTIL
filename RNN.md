# RNN

입력데이터 / 히든 영역 (hypothesis/ 가설) / 최종 y

입력값으로 나온 데이터만 쓰는게 아니라 이전 히든레이어에서 나온 것까지 해서 계산 ...없을 경우 0로 계산

ANN과 차이는 이전레이어의 히든레이어의 상태값을 활용하냐 여부 



#### RNN이 가진 문제 --> Gradient vanishing

모든 값들을 쓰려 하니까 그 전 데이터들을 잊는 문제

처음 나온 데이터를 까먹게 되는 문제



### LSTM

- 길게 기억할 것, 짧게 기억할 것을 나눠서 처리
- RNN은 탄젠트 하나만 사용... LSTM은 나온 값들을 메모리에 담는 작업까지.. GRU는 리셋게이트가 생기면서 아예 초기화 시키는 과정을 포함한다



